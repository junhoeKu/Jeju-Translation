{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8492096,"sourceType":"datasetVersion","datasetId":5066577}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["# !pip install -U accelerate\n","# !pip install -U transformers\n","\n","import accelerate\n","import transformers\n","import torch\n","import pandas as pd\n","import numpy as np\n","import os\n","from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW, BitsAndBytesConfig, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","from tqdm.notebook import tqdm\n","pd.options.display.max_colwidth = 1000\n","\n","## Check for CUDA availability\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-23T05:00:44.052366Z","iopub.execute_input":"2024-05-23T05:00:44.052979Z","iopub.status.idle":"2024-05-23T05:01:03.016392Z","shell.execute_reply.started":"2024-05-23T05:00:44.052948Z","shell.execute_reply":"2024-05-23T05:01:03.015389Z"},"trusted":true,"id":"HtUGMsTOMRLE","outputId":"482e8ff3-b96c-4bb6-8147-294d77211b8d"},"execution_count":null,"outputs":[{"name":"stderr","text":"2024-05-23 05:00:54.445955: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-23 05:00:54.446088: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-23 05:00:54.575077: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":["# Load model directly'\n","model_name = \"gogamza/kobart-base-v2\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"],"metadata":{"execution":{"iopub.status.busy":"2024-05-23T05:01:03.018049Z","iopub.execute_input":"2024-05-23T05:01:03.018639Z","iopub.status.idle":"2024-05-23T05:01:07.226914Z","shell.execute_reply.started":"2024-05-23T05:01:03.018612Z","shell.execute_reply":"2024-05-23T05:01:07.226131Z"},"trusted":true,"id":"9YbYbXfuMRLI","outputId":"2f36e133-ec93-4a90-a245-279d7d60e156","colab":{"referenced_widgets":["b486797dc0a748e79fdf1eb0634e17bc","49a3a1176ed7411a9257883f4e8100cc","a3552dd68dae45d9a448922dfa11b3b3","69d5a7ae20da4fb69d4afd714f32996b","f12ea8e780ee4d919829b7fa54ffda05"]}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.36k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b486797dc0a748e79fdf1eb0634e17bc"}},"metadata":{}},{"name":"stderr","text":"You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/682k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49a3a1176ed7411a9257883f4e8100cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/4.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3552dd68dae45d9a448922dfa11b3b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69d5a7ae20da4fb69d4afd714f32996b"}},"metadata":{}},{"name":"stderr","text":"You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\nYou passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/495M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f12ea8e780ee4d919829b7fa54ffda05"}},"metadata":{}}]},{"cell_type":"code","source":["train_data = pd.read_csv('0523_942292row.csv')"],"metadata":{"execution":{"iopub.status.busy":"2024-05-23T06:08:40.889182Z","iopub.execute_input":"2024-05-23T06:08:40.889528Z","iopub.status.idle":"2024-05-23T06:08:52.809871Z","shell.execute_reply.started":"2024-05-23T06:08:40.889500Z","shell.execute_reply":"2024-05-23T06:08:52.808763Z"},"trusted":true,"id":"gS-bcuU4MRLJ","outputId":"ab7fe1ce-cea6-4855-9ed2-0f8c9bd6434b"},"execution_count":null,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/1464057813.py:1: DtypeWarning: Columns (0,2,4,5,6,7,8,9,12,13) have mixed types. Specify dtype option on import or set low_memory=False.\n  train_data = pd.read_csv('/kaggle/input/0523-train2/0523_942292row.csv')\n","output_type":"stream"}]},{"cell_type":"code","source":["## 제주어 토큰, 표준어 토큰 정의하기\n","jeju_token = \"[제주]\"\n","standard_token = \"[표준]\"\n","\n","## 양방향 데이터 리스트 생성\n","bidirectional_data = []\n","\n","for dialect, standard in zip(train_data['dialect_form'], train_data['standard_form']):\n","    ## 토큰이 [제주] 일 경우 제주어 -> 표준어\n","    bidirectional_data.append({\n","        \"source\": jeju_token + \" \" + dialect,\n","        \"target\": standard\n","    })\n","    ## 토큰이 [표준] 일 경우 표준어 -> 제주어\n","    bidirectional_data.append({\n","        \"source\": standard_token + \" \" + standard,\n","        \"target\": dialect\n","    })\n","\n","## 데이터 토크나이징\n","tokenized_data = []\n","for item in bidirectional_data:\n","    source_encodings = tokenizer(item['source'], max_length=64, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n","    with tokenizer.as_target_tokenizer():\n","        target_encodings = tokenizer(item['target'], max_length=64, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n","    tokenized_data.append({\n","        \"input_ids\": source_encodings[\"input_ids\"],\n","        \"attention_mask\": source_encodings[\"attention_mask\"],\n","        \"labels\": target_encodings[\"input_ids\"]\n","    })"],"metadata":{"execution":{"iopub.status.busy":"2024-05-23T06:22:00.085115Z","iopub.execute_input":"2024-05-23T06:22:00.085552Z","iopub.status.idle":"2024-05-23T06:36:59.302703Z","shell.execute_reply.started":"2024-05-23T06:22:00.085502Z","shell.execute_reply":"2024-05-23T06:36:59.301516Z"},"trusted":true,"id":"56EFdTCJMRLJ","outputId":"b9f38896-da48-41de-e9d1-6c10ef5dc709"},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3935: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":["# !pip install datasets\n","from datasets import Dataset, DatasetDict\n","import pandas as pd\n","\n","# Modify the DataFrame creation code\n","## 요렇게 바꾸니까 메모리를 너무너무 아낄 수 있네 !!\n","formatted_data_df = pd.DataFrame([{\n","    \"input_ids\": np.array(fd[\"input_ids\"].numpy().tolist()[0], dtype=np.uint16),\n","    \"attention_mask\": np.array(fd[\"attention_mask\"].numpy().tolist()[0], dtype=np.uint8),\n","    \"labels\": np.array(fd[\"labels\"].numpy().tolist()[0], dtype=np.uint16)\n","} for fd in tokenized_data])\n","\n","## 데이터를 Dataset 형식으로 변환\n","train_dataset = Dataset.from_pandas(formatted_data_df)\n","\n","## 데이터셋 한번 확인해보슈\n","train_dataset"],"metadata":{"execution":{"iopub.status.busy":"2024-05-23T06:36:59.304958Z","iopub.execute_input":"2024-05-23T06:36:59.305426Z","iopub.status.idle":"2024-05-23T06:38:09.514448Z","shell.execute_reply.started":"2024-05-23T06:36:59.305388Z","shell.execute_reply":"2024-05-23T06:38:09.513464Z"},"trusted":true,"id":"muSuzthJMRLK","outputId":"edf6925a-a45e-4093-ad7d-c9dd3a18bf6e"},"execution_count":null,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 1884584\n})"},"metadata":{}}]},{"cell_type":"code","source":["## 학습 데이터셋을 학습 및 평가용으로 분리 (예: 90% 학습, 10% 평가)\n","train_test_split = train_dataset.train_test_split(test_size=0.1)\n","dataset_dict = DatasetDict({\n","    'train': train_test_split['train'],\n","    'test': train_test_split['test']\n","})"],"metadata":{"execution":{"iopub.status.busy":"2024-05-23T06:39:07.018384Z","iopub.execute_input":"2024-05-23T06:39:07.019126Z","iopub.status.idle":"2024-05-23T06:39:07.754090Z","shell.execute_reply.started":"2024-05-23T06:39:07.019096Z","shell.execute_reply":"2024-05-23T06:39:07.753037Z"},"trusted":true,"id":"8D95qq2HMRLL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n","\n","## 학습 매개변수 설정\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"checkpoints\",                           ## 결과물을 저장할 디렉토리\n","    evaluation_strategy=\"epoch\",                         ## 평가 전략\n","    learning_rate=2e-5,                                  ## 학습률\n","    per_device_train_batch_size=32,                      ## 디바이스 당 배치 크기\n","    weight_decay=0.01,                                   ## 가중치 감소\n","    save_total_limit=3,                                  ## 저장할 최대 체크포인트 수\n","    num_train_epochs=3,                                  ## 학습 에폭 수\n","    predict_with_generate=True,                          ## 생성을 사용한 예측 활성화\n",")"],"metadata":{"execution":{"iopub.status.busy":"2024-05-23T06:39:16.946559Z","iopub.execute_input":"2024-05-23T06:39:16.947401Z","iopub.status.idle":"2024-05-23T06:39:16.983844Z","shell.execute_reply.started":"2024-05-23T06:39:16.947371Z","shell.execute_reply":"2024-05-23T06:39:16.982749Z"},"trusted":true,"id":"OiVMfKxDMRLL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import wandb\n","\n","## API 키를 직접 입력\n","\n","wandb.login(key=\"\")\n","\n","## 학습 준비\n","trainer = Seq2SeqTrainer(\n","    model=model,                         ## 학습할 모델\n","    args=training_args,                  ## 학습 설정\n","    train_dataset=dataset_dict['train'], ## 학습 데이터셋\n","    eval_dataset=dataset_dict['test'],   ## 평가 데이터셋\n","    tokenizer=tokenizer\n",")\n","\n","## 학습 시작\n","trainer.train()\n","\n","## 학습 종료\n","wandb.finish()"],"metadata":{"execution":{"iopub.status.busy":"2024-05-23T06:39:19.796259Z","iopub.execute_input":"2024-05-23T06:39:19.796679Z","iopub.status.idle":"2024-05-23T06:39:31.321674Z","shell.execute_reply.started":"2024-05-23T06:39:19.796645Z","shell.execute_reply":"2024-05-23T06:39:31.320100Z"},"trusted":true,"id":"yZzxi_KqMRLL","outputId":"49805ca4-1e8d-457c-e686-906c83c1573e"},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='27' max='159012' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [    27/159012 00:06 < 12:10:55, 3.63 it/s, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 17\u001b[0m\n\u001b[1;32m      8\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                         \u001b[38;5;66;03m## 학습할 모델\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                  \u001b[38;5;66;03m## 학습 설정\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m## 학습 시작\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m## 학습 종료\u001b[39;00m\n\u001b[1;32m     20\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":[],"metadata":{"id":"bVxkCF4uMRLM"},"execution_count":null,"outputs":[]}]}
 
