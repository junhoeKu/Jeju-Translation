import json
import pandas as pd

# JSON 파일 경로
file_path = '../chatbot/Training/DZES20000002.json'

# 필요한 speaker_id, form, standard_form, dialect_form, isDialect 부분만 추출
def load_and_extract_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = json.load(file)

    utterances = data['utterance']
    extracted_data = []

    for utterance in utterances:
        speaker_id = utterance['speaker_id']
        form = utterance['form']
        standard_form = utterance['standard_form']
        dialect_form = utterance['dialect_form']
        isDialect = any(eojeol['isDialect'] for eojeol in utterance['eojeolList'])  # Check if any eojeol is a dialect

        utterance_data = {
            'speaker_id': speaker_id,
            'form': form,
            'standard_form': standard_form,
            'dialect_form': dialect_form,
            'isDialect': isDialect,
        }

        extracted_data.append(utterance_data)

    return extracted_data

# 데이터프레임화
extracted_data = load_and_extract_data(file_path)
df = pd.DataFrame(extracted_data)

import json
import pandas as pd
import os
import glob

# Define the function to process JSON files in a directory
def process_json_files(directory_path):
    # Use glob to find all JSON files in the directory
    json_files = glob.glob(os.path.join(directory_path, '*.json'))

    # Loop over each file path
    for file_path in json_files:
        # Use your existing function to load and extract data
        extracted_data = load_and_extract_data(file_path)

        # Create a DataFrame from the extracted data
        df = pd.DataFrame(extracted_data)

        # Here you can either return the DataFrame,
        # print it out, or perhaps write it to a CSV or another file
        print(f"Processed {file_path}")
        # For example, you could save each as a CSV
        df.to_csv(file_path.replace('.json', '.csv'), index=False)

        # If you want to combine all DataFrames into one, you could append them to a list
        # and concatenate them after the loop

    # If combining, return the concatenated DataFrame
    # return pd.concat(dfs_list, ignore_index=True)

# Path to the Training directory
training_dir_path = '../chatbot/Training'  # Replace with your actual directory path

# Execute the function
process_json_files(training_dir_path)

# Execute the function
process_json_files(training_dir_path)

import pandas as pd
import glob

# Replace with your actual directory path
training_dir_path = '../chatbot/Training'

# Use glob to list all CSV files in the directory
csv_files = glob.glob(os.path.join(training_dir_path, '*.csv'))

# Initialize an empty list to store DataFrames
dataframes = []

# Loop through all the files
for csv_file in csv_files:
    # Read the current CSV into a DataFrame
    df = pd.read_csv(csv_file)
    # Append the DataFrame to the list
    dataframes.append(df)

# Concatenate all the DataFrames into a single one
merged_df = pd.concat(dataframes, ignore_index=True)

# Let's define a function to process the standard_form and dialect_form columns as described
def preprocess_forms(row):
    # Split the sentence into tokens based on whitespace
    dialect_tokens = row['dialect_form'].split()
    standard_tokens = row['standard_form'].split()

    # For tokens that contain a '/', choose the part before '/' for dialect_form
    # and the part after '/' for standard_form
    row['dialect_form'] = ' '.join(token.split('/')[0] if '/' in token else token for token in dialect_tokens)
    row['standard_form'] = ' '.join(token.split('/')[1] if '/' in token else token for token in standard_tokens)

    return row

# Apply the preprocess function to each row in the DataFrame
processed_df = merged_df.apply(preprocess_forms, axis=1)
